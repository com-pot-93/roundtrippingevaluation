# roundtrippingevaluation


# ğŸ“ Repository Overview

In this repo we provide data, that was generated during the **evaluation** with the LLMs.  
Since execution of the same prompts on the same data, does not **guarantee** the same results, we provide all generated **artifacts** as they are after generation for the further usage and evaluation.

---

## ğŸ“‚ `generated_artefacts`

This folder contains data that was generated by the pipelines for each of 3 iterations:

- One file for each pipeline direction, for each used LLM, and for each dataset.

ğŸ” `generated_artefacts` are used to calculate data in [`iter_results`](#iter_results) (see `iteration_eval.py`)

---

## ğŸ“‚ `iter_results`

This folder contains results for each similarity metric after each iteration for each example (non-average):

- One file for each pipeline direction, for each used LLM, and for each dataset.

ğŸ” `iter_results` are used to calculate data in:

- ğŸ“„ `4.2 Stability` (`stability.py`)
- ğŸ“„ `results` (`average_iteration.py`)

---

## ğŸ“‚ `results`

This folder contains average data over 3 iterations for each similarity metric (average):

- One file for each pipeline direction, for each used LLM, and for each dataset.

ğŸ“Š `results` are used to calculate:

- Average data presented in **Tables 3 and 4**
- Spearman's correlation presented in **Tables 5â€“7**


