# roundtrippingevaluation

In this repo we provide data, that was generated during the evalution with the LLMs. 
Since execution of the same prompts on the same data, does not guaranty the same results, we provide all generated artefacts as they are after generation for the further usage and evaluation.

generated\_artefacts contains data, that was generated by the pipelines for each of 3 iterations;
1 file for each pipelinedirection, for each used llm, and for each dataset.
generated\_artefacts are used to calculate data in iter\_results (see iteration\_eval.py)

iter\_results contains results for each similarity metric after each iteration for each example (nonaverage);
1 file for each pipeline direction, for each used llm, and for each dataset.

iter\_results are used to calculate data in 4.2 Stability (stability.py)
iter\_results are used to calculate data in results (average\_iteration.py)

results contains average data over 3 iterations for each similarity metric (average);
1 file for each pipeline direction, for each used llm, and for each dataset.

results are used to calculate average data presented in Tables 3 and 4 and Spearmans correlation presented in Tables 5-7.
