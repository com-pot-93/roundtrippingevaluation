# roundtrippingevaluation


# 📁 Repository Overview

In this repo we provide data, that was generated during the **evaluation** with the LLMs.  
Since execution of the same prompts on the same data, does not **guarantee** the same results, we provide all generated **artifacts** as they are after generation for the further usage and evaluation.

---

## 📂 `generated_artefacts`

This folder contains data that was generated by the pipelines for each of 3 iterations:

- One file for each pipeline direction, for each used LLM, and for each dataset.

🔁 `generated_artefacts` are used to calculate data in [`iter_results`](#iter_results) (see `iteration_eval.py`)

---

## 📂 `iter_results`

This folder contains results for each similarity metric after each iteration for each example (non-average):

- One file for each pipeline direction, for each used LLM, and for each dataset.

🔍 `iter_results` are used to calculate data in:

- 📄 `4.2 Stability` (`stability.py`)
- 📄 `results` (`average_iteration.py`)

---

## 📂 `results`

This folder contains average data over 3 iterations for each similarity metric (average):

- One file for each pipeline direction, for each used LLM, and for each dataset.

📊 `results` are used to calculate:

- Average data presented in **Tables 3 and 4**
- Spearman's correlation presented in **Tables 5–7**


